

1.	itter Developer Account Keys:

 2.	HDFS Sink Configuration: 
a.	Ensure following jar file are present in path : (/usr/lib/flume-ng/lib)
  
b.	Configured Flume agent: 
 
Give following values in .conf file ( flume-twitter.conf  in /Desktop path) 
# spooldir.conf: A Spooling Directory Source

# Name the components on this agent
TwitterAgent.sources = Twitter 
TwitterAgent.channels = MemChannel 
TwitterAgent.sinks = HDFS
  
# Describing/Configuring the source 
TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource
TwitterAgent.sources.Twitter.consumerKey=
TwitterAgent.sources.Twitter.consumerSecret= 
TwitterAgent.sources.Twitter.accessToken=
TwitterAgent.sources.Twitter.accessTokenSecret= TwitterAgent.sources.Twitter.keywords=hadoop, bigdata, mapreduce, mahout, hbase, nosql, election,sports, cricket
TwitterAgent.sources.Twitter.channels=MemChannel
# Describing/Configuring the sink 

TwitterAgent.sinks.HDFS.channel=MemChannel
TwitterAgent.sinks.HDFS.type=hdfs
TwitterAgent.sinks.HDFS.hdfs.path=/loudacre/twitter
TwitterAgent.sinks.HDFS.hdfs.fileType=DataStream
TwitterAgent.sinks.HDFS.hdfs.writeformat=Text
TwitterAgent.sinks.HDFS.hdfs.batchSize=100
TwitterAgent.sinks.HDFS.hdfs.rollSize=0
TwitterAgent.sinks.HDFS.hdfs.rollCount=100
TwitterAgent.sinks.HDFS.hdfs.rollInterval=100
TwitterAgent.sinks.HDFS.hdfs.useLocalTimeStamp=true

TwitterAgent.channels.MemChannel.type=memory
TwitterAgent.channels.MemChannel.capacity=1000
TwitterAgent.channels.MemChannel.transactionCapacity=1000

c.	Make sure HDFS directories are present : 
sudo -u hdfs hadoop fs -mkdir /loudacre/twitter

d.	Running the Flume Agent: 
flume-ng agent --conf /etc/flume-ng/conf \
--conf-file Desktop/flume-twitter.conf \
--name TwitterAgent -Dflume.root.logger=INFO,console

e.	Output Directories : 

Data: 

3.	Solr Configuration: 
Follow the steps as mentioned: 
a.	Ensure following jar file are present in path : (/usr/lib/flume-ng/lib)

b.	Configuring Solr: 
Go to path etc/default/   and set follosing properties as described below. 
SOLR_HDFS_HOME=hdfs://localhost:8020/solr
SOLR_HDFS_CONFIG=/etc/hadoop/conf
/etc/Hadoop/conf should have core-site.xml and hdfs-site.xml. files 
 
c.	 Checking the /solr Directory in HDFS
Before starting the Cloudera Search server, you need to create the /solr directory in HDFS. 
If they are not created, run following command in hdfs : 
sudo -u hdfs hadoop fs -mkdir /solr
sudo -u hdfs hadoop fs -chown solr /solr
d.	Initializing the ZooKeeper Namespace
Before starting the Cloudera Search server, you need to create the solr namespace in ZooKeeper:
$ solrctl init
 
e.	Starting Solr

$ sudo service solr-server restart
 
f.	Runtime Solr Configuration
To start using Solr for indexing the data, you must configure a collection holding the index. A configuration for a collection requires a solrconfig.xml file, a schema.xml and any helper files referenced from the xml files. The solrconfig.xml file contains all of the Solr settings for a given collection, and the schema.xml file specifies the schema that Solr uses when indexing documents. 
Configuration files for a collection are managed as part of the instance directory. To generate a skeleton of the instance directory, run the following command:
$ solrctl instancedir --generate $HOME/solr_configs
 
Generate the configuration files for the collection, including the tweet specific schema.xml:

cp /usr/share/doc/search*/examples/solr-nrt/collection1/conf/schema.xml \
$HOME/solr_configs/conf

After configuration is complete, you can make it available to Solr by issuing the following command, which uploads the content of the entire instance directory to ZooKeeper:
solrctl instancedir --create collection1 $HOME/solr_configs

Create the new collection:
$ solrctl collection --create collection1 -s 1 -c collection1


Verify the collection is live. For example, for the localhost, use http://localhost:8983/solr/#/~cloud
 

Keep solr server up and running.

g.	Flume agent configuration : 

Make sure following file are in path and properly configured. 
Morphline.conf (/etc/flume-ng/conf)
Run following command : 
cp /usr/share/doc/search-1.0.0+cdh5.4.3+0/quickstart/morphline.conf /etc/flume-ng/conf

solr-twitter.conf (Desktop)
 
# spooldir.conf: A Spooling Directory Source
# Name the components on this agent
TwitterAgent.sources = Twitter 
TwitterAgent.channels = MemChannel 
TwitterAgent.sinks = solrSink
  
# Describing/Configuring the source 
TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource
TwitterAgent.sources.Twitter.consumerKey= 
TwitterAgent.sources.Twitter.consumerSecret=
 TwitterAgent.sources.Twitter.accessToken=
TwitterAgent.sources.Twitter.accessTokenSecret=
 TwitterAgent.sources.Twitter.keywords=hadoop, bigdata, mapreduce, mahout, hbase, nosql, election,sports, cricket
TwitterAgent.sources.Twitter.channels=MemChannel
# Describing/Configuring the sink 

TwitterAgent.sinks.solrSink.type = org.apache.flume.sink.solr.morphline.MorphlineSolrSink
TwitterAgent.sinks.solrSink.channel = MemChannel
#TwitterAgent.sinks.solrSink.batchSize = 1000
#TwitterAgent.sinks.solrSink.batchDurationMillis = 1000
TwitterAgent.sinks.solrSink.morphlineFile = /etc/flume-ng/conf/morphline.conf
TwitterAgent.sinks.solrSink.morphlineId = morphline1

TwitterAgent.channels.MemChannel.type=memory
TwitterAgent.channels.MemChannel.capacity=1000
TwitterAgent.channels.MemChannel.transactionCapacity=1000

Flume-env.sh (/etc/flume-ng/conf)
Add following line into file : 
export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote"

h.	Starting flume agent 
flume-ng agent --conf /etc/flume-ng/conf \
--conf-file Desktop/solr-twitter.conf \
--name TwitterAgent -Dflume.root.logger=INFO,console

i.	Following the files which are created on HDFS /solr path : 

Index : 

Data: 
 

 

 


